{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXpUjES4tSPI"
      },
      "source": [
        "PNN technique for data optimization and"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXs4gfweSCIH",
        "outputId": "449dc192-2cca-48da-b0f2-c227000a81ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.7/dist-packages (0.15.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (0.12.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (1.15.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (1.3.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (4.4.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (0.1.6)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (1.19.5)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (0.4.0)\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow-probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XeeR4NgMKzU",
        "outputId": "4bdde346-9c78-4982-dcb5-cee73983bd11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (4.0.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (0.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.19.5)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (21.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (0.16.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.15.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (5.4.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (0.12.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.4.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.1.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (0.3.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (4.62.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.10)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->tensorflow-datasets) (3.6.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.53.0)\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow-datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "372aiez6OStW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3qg24KDOWkp"
      },
      "outputs": [],
      "source": [
        "def get_train_and_test_splits(train_size, batch_size=1):\n",
        "    # Prefetch with a buffer the same size as the dataset because th dataset is very small and fits into memory.\n",
        "    dataset = (\n",
        "        tfds.load(name=\"wine_quality\", as_supervised=True, split=\"train\")\n",
        "        .map(lambda x, y: (x, tf.cast(y, tf.float32)))\n",
        "        .prefetch(buffer_size=dataset_size)\n",
        "        .cache()\n",
        "    )\n",
        "    # Shuffle with a buffer the same size as the dataset.\n",
        "    train_dataset = (\n",
        "        dataset.take(train_size).shuffle(buffer_size=train_size).batch(batch_size)\n",
        "    )\n",
        "    test_dataset = dataset.skip(train_size).batch(batch_size)\n",
        "\n",
        "    return train_dataset, test_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPWBZQ_eOeen"
      },
      "outputs": [],
      "source": [
        "hidden_units = [8, 8]\n",
        "learning_rate = 0.001\n",
        "\n",
        "\n",
        "def run_experiment(model, loss, train_dataset, test_dataset):\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.RMSprop(learning_rate=learning_rate),\n",
        "        loss=loss,\n",
        "        metrics=[keras.metrics.RootMeanSquaredError()],\n",
        "    )\n",
        "\n",
        "    print(\"Start training the model...\")\n",
        "    model.fit(train_dataset, epochs=num_epochs, validation_data=test_dataset)\n",
        "    print(\"Model training finished.\")\n",
        "    _, rmse = model.evaluate(train_dataset, verbose=0)\n",
        "    print(f\"Train RMSE: {round(rmse, 3)}\")\n",
        "\n",
        "    print(\"Evaluating model performance...\")\n",
        "    _, rmse = model.evaluate(test_dataset, verbose=0)\n",
        "    print(f\"Test RMSE: {round(rmse, 3)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYGTJs-rOkAJ"
      },
      "outputs": [],
      "source": [
        "FEATURE_NAMES = [\n",
        "    \"fixed acidity\",\n",
        "    \"volatile acidity\",\n",
        "    \"citric acid\",\n",
        "    \"residual sugar\",\n",
        "    \"chlorides\",\n",
        "    \"free sulfur dioxide\",\n",
        "    \"total sulfur dioxide\",\n",
        "    \"density\",\n",
        "    \"pH\",\n",
        "    \"sulphates\",\n",
        "    \"alcohol\",\n",
        "]\n",
        "\n",
        "\n",
        "def create_model_inputs():\n",
        "    inputs = {}\n",
        "    for feature_name in FEATURE_NAMES:\n",
        "        inputs[feature_name] = layers.Input(\n",
        "            name=feature_name, shape=(1,), dtype=tf.float32\n",
        "        )\n",
        "    return inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6QjBcX1PMKp"
      },
      "source": [
        "Creating probability neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSsFO2VoPEoW"
      },
      "outputs": [],
      "source": [
        "def create_baseline_model():\n",
        "    inputs = create_model_inputs()\n",
        "    input_values = [value for _, value in sorted(inputs.items())]\n",
        "    features = keras.layers.concatenate(input_values)\n",
        "    features = layers.BatchNormalization()(features)\n",
        "\n",
        "    # Create hidden layers with deterministic weights using the Dense layer.\n",
        "    for units in hidden_units:\n",
        "        features = layers.Dense(units, activation=\"sigmoid\")(features)\n",
        "    # The output is deterministic: a single point estimate.\n",
        "    outputs = layers.Dense(units=1)(features)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4_sctWRRR7G"
      },
      "outputs": [],
      "source": [
        "dataset_size = 4898\n",
        "batch_size = 256\n",
        "train_size = int(dataset_size * 0.85)\n",
        "train_dataset, test_dataset = get_train_and_test_splits(train_size, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FO_wH9nORUKV",
        "outputId": "6bc3d429-ca41-4727-ddd2-9ab51d2b2619"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training the model...\n",
            "Epoch 1/10\n",
            "17/17 [==============================] - 2s 65ms/step - loss: 29.4161 - root_mean_squared_error: 5.4237 - val_loss: 26.3928 - val_root_mean_squared_error: 5.1374\n",
            "Epoch 2/10\n",
            "17/17 [==============================] - 0s 10ms/step - loss: 27.6047 - root_mean_squared_error: 5.2540 - val_loss: 25.2274 - val_root_mean_squared_error: 5.0227\n",
            "Epoch 3/10\n",
            "17/17 [==============================] - 0s 11ms/step - loss: 26.0760 - root_mean_squared_error: 5.1065 - val_loss: 24.0143 - val_root_mean_squared_error: 4.9004\n",
            "Epoch 4/10\n",
            "17/17 [==============================] - 0s 11ms/step - loss: 24.6159 - root_mean_squared_error: 4.9614 - val_loss: 22.7766 - val_root_mean_squared_error: 4.7725\n",
            "Epoch 5/10\n",
            "17/17 [==============================] - 0s 11ms/step - loss: 23.1953 - root_mean_squared_error: 4.8162 - val_loss: 21.5358 - val_root_mean_squared_error: 4.6407\n",
            "Epoch 6/10\n",
            "17/17 [==============================] - 0s 12ms/step - loss: 21.8138 - root_mean_squared_error: 4.6705 - val_loss: 20.3028 - val_root_mean_squared_error: 4.5059\n",
            "Epoch 7/10\n",
            "17/17 [==============================] - 0s 11ms/step - loss: 20.4722 - root_mean_squared_error: 4.5246 - val_loss: 19.0964 - val_root_mean_squared_error: 4.3699\n",
            "Epoch 8/10\n",
            "17/17 [==============================] - 0s 12ms/step - loss: 19.1708 - root_mean_squared_error: 4.3784 - val_loss: 17.9275 - val_root_mean_squared_error: 4.2341\n",
            "Epoch 9/10\n",
            "17/17 [==============================] - 0s 11ms/step - loss: 17.9104 - root_mean_squared_error: 4.2321 - val_loss: 16.7801 - val_root_mean_squared_error: 4.0964\n",
            "Epoch 10/10\n",
            "17/17 [==============================] - 0s 11ms/step - loss: 16.6939 - root_mean_squared_error: 4.0858 - val_loss: 15.6687 - val_root_mean_squared_error: 3.9584\n",
            "Model training finished.\n",
            "Train RMSE: 3.993\n",
            "Evaluating model performance...\n",
            "Test RMSE: 3.958\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "mse_loss = keras.losses.MeanSquaredError()\n",
        "baseline_model = create_baseline_model()\n",
        "run_experiment(baseline_model, mse_loss, train_dataset, test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFB_YypXRW2O",
        "outputId": "2eebbc3e-962d-45ca-a781-cbcacb821cc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted: 1.9 - Actual: 4.0\n",
            "Predicted: 2.0 - Actual: 6.0\n",
            "Predicted: 2.0 - Actual: 5.0\n",
            "Predicted: 2.0 - Actual: 6.0\n",
            "Predicted: 2.1 - Actual: 5.0\n",
            "Predicted: 2.0 - Actual: 6.0\n",
            "Predicted: 2.0 - Actual: 5.0\n",
            "Predicted: 2.1 - Actual: 5.0\n",
            "Predicted: 2.0 - Actual: 5.0\n",
            "Predicted: 1.9 - Actual: 6.0\n"
          ]
        }
      ],
      "source": [
        "sample = 10\n",
        "examples, targets = list(test_dataset.unbatch().shuffle(batch_size * 10).batch(sample))[\n",
        "    0\n",
        "]\n",
        "\n",
        "predicted = baseline_model(examples).numpy()\n",
        "for idx in range(sample):\n",
        "    print(f\"Predicted: {round(float(predicted[idx][0]), 1)} - Actual: {targets[idx]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTVp9QMYReHG"
      },
      "source": [
        "BNN moidel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIyb7chRRdMm"
      },
      "outputs": [],
      "source": [
        "\n",
        "def prior(kernel_size, bias_size, dtype=None):\n",
        "    n = kernel_size + bias_size\n",
        "    prior_model = keras.Sequential(\n",
        "        [\n",
        "            tfp.layers.DistributionLambda(\n",
        "                lambda t: tfp.distributions.MultivariateNormalDiag(\n",
        "                    loc=tf.zeros(n), scale_diag=tf.ones(n)\n",
        "                )\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "    return prior_model\n",
        "\n",
        "\n",
        "\n",
        "def posterior(kernel_size, bias_size, dtype=None):\n",
        "    n = kernel_size + bias_size\n",
        "    posterior_model = keras.Sequential(\n",
        "        [\n",
        "            tfp.layers.VariableLayer(\n",
        "                tfp.layers.MultivariateNormalTriL.params_size(n), dtype=dtype\n",
        "            ),\n",
        "            tfp.layers.MultivariateNormalTriL(n),\n",
        "        ]\n",
        "    )\n",
        "    return posterior_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ugoiUBdRf5F"
      },
      "outputs": [],
      "source": [
        "def create_bnn_model(train_size):\n",
        "    inputs = create_model_inputs()\n",
        "    features = keras.layers.concatenate(list(inputs.values()))\n",
        "    features = layers.BatchNormalization()(features)\n",
        "\n",
        "    # Create hidden layers with weight uncertainty using the DenseVariational layer.\n",
        "    for units in hidden_units:\n",
        "        features = tfp.layers.DenseVariational(\n",
        "            units=units,\n",
        "            make_prior_fn=prior,\n",
        "            make_posterior_fn=posterior,\n",
        "            kl_weight=1 / train_size,\n",
        "            activation=\"sigmoid\",\n",
        "        )(features)\n",
        "\n",
        "    # The output is deterministic: a single point estimate.\n",
        "    outputs = layers.Dense(units=1)(features)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAZom7MqRk6d",
        "outputId": "5db071cc-388e-4a82-e2e5-23674de1cf49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training the model...\n",
            "Epoch 1/10\n",
            "5/5 [==============================] - 3s 235ms/step - loss: 30.4242 - root_mean_squared_error: 5.5144 - val_loss: 23.6098 - val_root_mean_squared_error: 4.8574\n",
            "Epoch 2/10\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 30.0407 - root_mean_squared_error: 5.4799 - val_loss: 30.2720 - val_root_mean_squared_error: 5.5007\n",
            "Epoch 3/10\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 29.6040 - root_mean_squared_error: 5.4397 - val_loss: 31.0157 - val_root_mean_squared_error: 5.5679\n",
            "Epoch 4/10\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 29.0708 - root_mean_squared_error: 5.3907 - val_loss: 30.0951 - val_root_mean_squared_error: 5.4843\n",
            "Epoch 5/10\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 29.7677 - root_mean_squared_error: 5.4549 - val_loss: 21.6480 - val_root_mean_squared_error: 4.6513\n",
            "Epoch 6/10\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 30.8822 - root_mean_squared_error: 5.5559 - val_loss: 27.4118 - val_root_mean_squared_error: 5.2341\n",
            "Epoch 7/10\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 29.9316 - root_mean_squared_error: 5.4695 - val_loss: 24.8912 - val_root_mean_squared_error: 4.9878\n",
            "Epoch 8/10\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 26.5583 - root_mean_squared_error: 5.1522 - val_loss: 25.4169 - val_root_mean_squared_error: 5.0399\n",
            "Epoch 9/10\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 26.6475 - root_mean_squared_error: 5.1608 - val_loss: 21.4539 - val_root_mean_squared_error: 4.6304\n",
            "Epoch 10/10\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 27.6815 - root_mean_squared_error: 5.2600 - val_loss: 23.4425 - val_root_mean_squared_error: 4.8397\n",
            "Model training finished.\n",
            "Train RMSE: 4.987\n",
            "Evaluating model performance...\n",
            "Test RMSE: 5.171\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "train_sample_size = int(train_size * 0.3)\n",
        "small_train_dataset = train_dataset.unbatch().take(train_sample_size).batch(batch_size)\n",
        "\n",
        "bnn_model_small = create_bnn_model(train_sample_size)\n",
        "run_experiment(bnn_model_small, mse_loss, small_train_dataset, test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxeB3toBRoc2",
        "outputId": "e2e37301-a5fe-4e58-8f07-fc02037ff4ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions mean: 0.85, min: 0.03, max: 1.68, range: 1.64 - Actual: 4.0\n",
            "Predictions mean: 0.84, min: 0.0, max: 1.59, range: 1.59 - Actual: 6.0\n",
            "Predictions mean: 0.85, min: 0.0, max: 1.55, range: 1.55 - Actual: 5.0\n",
            "Predictions mean: 0.85, min: 0.03, max: 1.53, range: 1.51 - Actual: 6.0\n",
            "Predictions mean: 0.86, min: -0.0, max: 1.58, range: 1.58 - Actual: 5.0\n",
            "Predictions mean: 0.84, min: 0.03, max: 1.56, range: 1.54 - Actual: 6.0\n",
            "Predictions mean: 0.85, min: 0.05, max: 1.64, range: 1.59 - Actual: 5.0\n",
            "Predictions mean: 0.87, min: -0.01, max: 1.58, range: 1.59 - Actual: 5.0\n",
            "Predictions mean: 0.86, min: -0.0, max: 1.55, range: 1.55 - Actual: 5.0\n",
            "Predictions mean: 0.84, min: 0.03, max: 1.56, range: 1.54 - Actual: 6.0\n"
          ]
        }
      ],
      "source": [
        "def compute_predictions(model, iterations=100):\n",
        "    predicted = []\n",
        "    for _ in range(iterations):\n",
        "        predicted.append(model(examples).numpy())\n",
        "    predicted = np.concatenate(predicted, axis=1)\n",
        "\n",
        "    prediction_mean = np.mean(predicted, axis=1).tolist()\n",
        "    prediction_min = np.min(predicted, axis=1).tolist()\n",
        "    prediction_max = np.max(predicted, axis=1).tolist()\n",
        "    prediction_range = (np.max(predicted, axis=1) - np.min(predicted, axis=1)).tolist()\n",
        "\n",
        "    for idx in range(sample):\n",
        "        print(\n",
        "            f\"Predictions mean: {round(prediction_mean[idx], 2)}, \"\n",
        "            f\"min: {round(prediction_min[idx], 2)}, \"\n",
        "            f\"max: {round(prediction_max[idx], 2)}, \"\n",
        "            f\"range: {round(prediction_range[idx], 2)} - \"\n",
        "            f\"Actual: {targets[idx]}\"\n",
        "        )\n",
        "\n",
        "\n",
        "compute_predictions(bnn_model_small)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-BWfqymRq9d",
        "outputId": "147a576f-e42e-48f8-f113-1ee765663b1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training the model...\n",
            "Epoch 1/10\n",
            "17/17 [==============================] - 3s 58ms/step - loss: 21.8791 - root_mean_squared_error: 4.6771 - val_loss: 18.4855 - val_root_mean_squared_error: 4.2989\n",
            "Epoch 2/10\n",
            "17/17 [==============================] - 0s 14ms/step - loss: 20.1557 - root_mean_squared_error: 4.4890 - val_loss: 23.2813 - val_root_mean_squared_error: 4.8247\n",
            "Epoch 3/10\n",
            "17/17 [==============================] - 0s 13ms/step - loss: 18.8150 - root_mean_squared_error: 4.3371 - val_loss: 14.6785 - val_root_mean_squared_error: 3.8307\n",
            "Epoch 4/10\n",
            "17/17 [==============================] - 0s 13ms/step - loss: 19.5054 - root_mean_squared_error: 4.4159 - val_loss: 15.0795 - val_root_mean_squared_error: 3.8826\n",
            "Epoch 5/10\n",
            "17/17 [==============================] - 0s 12ms/step - loss: 17.0479 - root_mean_squared_error: 4.1284 - val_loss: 18.1559 - val_root_mean_squared_error: 4.2604\n",
            "Epoch 6/10\n",
            "17/17 [==============================] - 0s 12ms/step - loss: 17.5731 - root_mean_squared_error: 4.1916 - val_loss: 18.5077 - val_root_mean_squared_error: 4.3014\n",
            "Epoch 7/10\n",
            "17/17 [==============================] - 0s 12ms/step - loss: 15.3195 - root_mean_squared_error: 3.9134 - val_loss: 14.0436 - val_root_mean_squared_error: 3.7469\n",
            "Epoch 8/10\n",
            "17/17 [==============================] - 0s 13ms/step - loss: 15.2448 - root_mean_squared_error: 3.9038 - val_loss: 11.5354 - val_root_mean_squared_error: 3.3956\n",
            "Epoch 9/10\n",
            "17/17 [==============================] - 0s 13ms/step - loss: 13.7203 - root_mean_squared_error: 3.7034 - val_loss: 9.8178 - val_root_mean_squared_error: 3.1325\n",
            "Epoch 10/10\n",
            "17/17 [==============================] - 0s 12ms/step - loss: 11.6452 - root_mean_squared_error: 3.4118 - val_loss: 13.7784 - val_root_mean_squared_error: 3.7112\n",
            "Model training finished.\n",
            "Train RMSE: 3.152\n",
            "Evaluating model performance...\n",
            "Test RMSE: 3.511\n",
            "Predictions mean: 2.61, min: 1.35, max: 3.57, range: 2.22 - Actual: 4.0\n",
            "Predictions mean: 2.61, min: 1.47, max: 3.66, range: 2.19 - Actual: 6.0\n",
            "Predictions mean: 2.63, min: 1.34, max: 3.64, range: 2.3 - Actual: 5.0\n",
            "Predictions mean: 2.64, min: 1.35, max: 3.64, range: 2.29 - Actual: 6.0\n",
            "Predictions mean: 2.61, min: 1.3, max: 3.67, range: 2.37 - Actual: 5.0\n",
            "Predictions mean: 2.65, min: 1.32, max: 3.67, range: 2.35 - Actual: 6.0\n",
            "Predictions mean: 2.62, min: 1.32, max: 3.52, range: 2.2 - Actual: 5.0\n",
            "Predictions mean: 2.63, min: 1.19, max: 3.63, range: 2.44 - Actual: 5.0\n",
            "Predictions mean: 2.63, min: 1.37, max: 3.66, range: 2.29 - Actual: 5.0\n",
            "Predictions mean: 2.62, min: 1.47, max: 3.66, range: 2.19 - Actual: 6.0\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "bnn_model_full = create_bnn_model(train_size)\n",
        "run_experiment(bnn_model_full, mse_loss, train_dataset, test_dataset)\n",
        "\n",
        "compute_predictions(bnn_model_full)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSEax_KMPrh9"
      },
      "source": [
        "PNN model and training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50Oqv1ZpPqwH"
      },
      "outputs": [],
      "source": [
        "def create_probablistic_bnn_model(train_size):\n",
        "    inputs = create_model_inputs()\n",
        "    features = keras.layers.concatenate(list(inputs.values()))\n",
        "    features = layers.BatchNormalization()(features)\n",
        "\n",
        "    # Create hidden layers with weight uncertainty using the DenseVariational layer.\n",
        "    for units in hidden_units:\n",
        "        features = tfp.layers.DenseVariational(\n",
        "            units=units,\n",
        "            make_prior_fn=prior,\n",
        "            make_posterior_fn=posterior,\n",
        "            kl_weight=1 / train_size,\n",
        "            activation=\"sigmoid\",\n",
        "        )(features)\n",
        "\n",
        "    # Create a probabilistic output (Normal distribution), and use the `Dense` layer\n",
        "    # to produce the parameters of the distribution.\n",
        "    # Set units=2 to learn both the mean and the variance of the Normal distribution.\n",
        "    distribution_params = layers.Dense(units=2)(features)\n",
        "    outputs = tfp.layers.IndependentNormal(1)(distribution_params)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFblP9f-Pw_G",
        "outputId": "bebc224e-6fa1-41ef-9a72-ddc9f4342ac4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training the model...\n",
            "Epoch 1/10\n",
            "17/17 [==============================] - 4s 66ms/step - loss: 212.9756 - root_mean_squared_error: 6.4623 - val_loss: 139.8549 - val_root_mean_squared_error: 6.5212\n",
            "Epoch 2/10\n",
            "17/17 [==============================] - 0s 13ms/step - loss: 184.7269 - root_mean_squared_error: 6.4528 - val_loss: 220.9004 - val_root_mean_squared_error: 6.3506\n",
            "Epoch 3/10\n",
            "17/17 [==============================] - 0s 12ms/step - loss: 176.9464 - root_mean_squared_error: 6.3008 - val_loss: 204.8843 - val_root_mean_squared_error: 6.3638\n",
            "Epoch 4/10\n",
            "17/17 [==============================] - 0s 15ms/step - loss: 135.0159 - root_mean_squared_error: 6.1718 - val_loss: 85.2571 - val_root_mean_squared_error: 5.9879\n",
            "Epoch 5/10\n",
            "17/17 [==============================] - 0s 13ms/step - loss: 99.8310 - root_mean_squared_error: 6.1422 - val_loss: 92.1866 - val_root_mean_squared_error: 6.3537\n",
            "Epoch 6/10\n",
            "17/17 [==============================] - 0s 13ms/step - loss: 115.9007 - root_mean_squared_error: 6.1636 - val_loss: 49.8186 - val_root_mean_squared_error: 5.8316\n",
            "Epoch 7/10\n",
            "17/17 [==============================] - 0s 15ms/step - loss: 85.8623 - root_mean_squared_error: 5.9011 - val_loss: 58.7284 - val_root_mean_squared_error: 6.0452\n",
            "Epoch 8/10\n",
            "17/17 [==============================] - 0s 13ms/step - loss: 63.9102 - root_mean_squared_error: 5.9220 - val_loss: 59.2070 - val_root_mean_squared_error: 5.7403\n",
            "Epoch 9/10\n",
            "17/17 [==============================] - 0s 13ms/step - loss: 43.0941 - root_mean_squared_error: 5.7331 - val_loss: 30.8746 - val_root_mean_squared_error: 5.8855\n",
            "Epoch 10/10\n",
            "17/17 [==============================] - 0s 13ms/step - loss: 52.3746 - root_mean_squared_error: 5.7984 - val_loss: 42.0797 - val_root_mean_squared_error: 5.7018\n",
            "Model training finished.\n",
            "Train RMSE: 5.792\n",
            "Evaluating model performance...\n",
            "Test RMSE: 5.454\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def negative_loglikelihood(targets, estimated_distribution):\n",
        "    return -estimated_distribution.log_prob(targets)\n",
        "\n",
        "\n",
        "num_epochs = 10\n",
        "prob_bnn_model = create_probablistic_bnn_model(train_size)\n",
        "run_experiment(prob_bnn_model, negative_loglikelihood, train_dataset, test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLr1xn_mPyBK",
        "outputId": "78556832-d6d5-4547-928c-d7fe0507564e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction mean: -0.16, stddev: 0.78, 95% CI: [1.36 - -1.68] - Actual: 4.0\n",
            "Prediction mean: -0.11, stddev: 0.82, 95% CI: [1.49 - -1.71] - Actual: 6.0\n",
            "Prediction mean: 0.08, stddev: 0.79, 95% CI: [1.62 - -1.46] - Actual: 5.0\n",
            "Prediction mean: 0.13, stddev: 0.71, 95% CI: [1.53 - -1.27] - Actual: 6.0\n",
            "Prediction mean: 0.14, stddev: 0.83, 95% CI: [1.76 - -1.49] - Actual: 5.0\n",
            "Prediction mean: -0.0, stddev: 0.73, 95% CI: [1.42 - -1.42] - Actual: 6.0\n",
            "Prediction mean: -0.13, stddev: 0.78, 95% CI: [1.41 - -1.67] - Actual: 5.0\n",
            "Prediction mean: 0.13, stddev: 0.86, 95% CI: [1.81 - -1.56] - Actual: 5.0\n",
            "Prediction mean: 0.11, stddev: 0.83, 95% CI: [1.74 - -1.52] - Actual: 5.0\n",
            "Prediction mean: -0.1, stddev: 0.78, 95% CI: [1.42 - -1.63] - Actual: 6.0\n"
          ]
        }
      ],
      "source": [
        "prediction_distribution = prob_bnn_model(examples)\n",
        "prediction_mean = prediction_distribution.mean().numpy().tolist()\n",
        "prediction_stdv = prediction_distribution.stddev().numpy()\n",
        "\n",
        "# The 95% CI is computed as mean Â± (1.96 * stdv)\n",
        "upper = (prediction_mean + (1.96 * prediction_stdv)).tolist()\n",
        "lower = (prediction_mean - (1.96 * prediction_stdv)).tolist()\n",
        "prediction_stdv = prediction_stdv.tolist()\n",
        "\n",
        "for idx in range(sample):\n",
        "    print(\n",
        "        f\"Prediction mean: {round(prediction_mean[idx][0], 2)}, \"\n",
        "        f\"stddev: {round(prediction_stdv[idx][0], 2)}, \"\n",
        "        f\"95% CI: [{round(upper[idx][0], 2)} - {round(lower[idx][0], 2)}]\"\n",
        "        f\" - Actual: {targets[idx]}\"\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ML Project(PNN).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
